{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center><font size=6>CS490: Machine Learning<br>Homework 1</font></center>**\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "<hr>\n",
    "\n",
    "In order to reduce my email load, I decide to implement a machine learning algorithm to decide whether or not I should read an email, or simply file it away instead.  To train my model, I obtain the following data set of binary-valued features about each email,\n",
    "including whether I know the author or not, whether the email is long or short, and whether it has any of several key words, along\n",
    "with my final decision about whether to read it (y=+1 for \"read\", y=-1 for \"discard\").\n",
    "\n",
    "| know author? | is long? | has 'research' | has 'grade' | has 'lottery' | read? |\n",
    "|--------------|----------|----------------|-------------|---------------|-------|\n",
    "| 0 | 0 | 1 | 1 | 0 | -1 |\n",
    "| 1 | 1 | 0 | 1 | 0 | -1 |\n",
    "| 0 | 1 | 1 | 1 | 1 | -1 |\n",
    "| 1 | 1 | 1 | 1 | 0 | -1 |\n",
    "| 0 | 1 | 0 | 0 | 0 | -1 |\n",
    "| 1 | 0 | 1 | 1 | 1 | 1 |\n",
    "| 0 | 0 | 1 | 0 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 0 | 0 | 1 |\n",
    "| 1 | 0 | 1 | 1 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 1 | 1 | -1 |\n",
    "\n",
    "I decide to try a naive Bayes classifier to make my decisions and compute my uncertainty.  In the case of any ties where both classes have equal probability, we will prefer to predict class +1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Naive Bayes prior and likelihoods\n",
    "\n",
    "Compute all the probabilities necessary for a discrete Naive Bayes classifier, i.e.,\n",
    "the class probability p(y) and all the individual feature probabilities p(x_i|y), for each class y and feature x_i. \n",
    "\n",
    "**NOTE: There is not an mltools utility for this--you need to do the computations on your own. However, you can do the counting by hand and just fill in the fractions for each parameter. For example, if in the data above you count that the parameter for p(x_i | y=1) = 2/3, you will just create a a variable below and set it equal to the fraction:**\n",
    "\n",
    "```p_xi_y1 = 2.0 / 3```\n",
    "\n",
    "You will want these variables to be floating points for use below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate your naive Bayes parameters here \n",
    "\n",
    "# Priors\n",
    "\n",
    "# We have y as our conclusion\n",
    "p_y = 4.0/10.0     # keep email? = true\n",
    "\n",
    "# Conditional Probabilities\n",
    "\n",
    "# x0 table\n",
    "p_x0_y1 = 3.0/4.0\n",
    "p_x0_y0 = 3.0/6.0\n",
    "\n",
    "# x1 table\n",
    "p_x1_y1 = 0.0/4.0\n",
    "p_x1_y0 = 5.0/6.0\n",
    "\n",
    "# x2 table\n",
    "p_x2_y1 = 3.0/4.0\n",
    "p_x2_y0 = 4.0/6.0\n",
    "\n",
    "# x3 table\n",
    "p_x3_y1 = 2.0/4.0\n",
    "p_x3_y0 = 5.0/6.0\n",
    "\n",
    "# x4 table\n",
    "p_x4_y1 = 1.0/4.0\n",
    "p_x4_y0 = 2.0/6.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Prediction\n",
    "Use your learned parameters above to predict whether emails with the following features would be kept or discarded:\n",
    "\n",
    "1. Short email from an unknown sender that isn't about research, grades, or a lottery. \n",
    "2. ***x*** = (1, 1, 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 1: keep \n",
      "\n",
      "Problem 2: discard \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Compute the predictions for the above email features.\n",
    "\n",
    "# problem 1: x = (0, 0, 0, 0, 0)\n",
    "keep = p_y * (1-p_x0_y1) * (1-p_x1_y1) * (1-p_x2_y1) * (1-p_x3_y1) * (1-p_x4_y1)\n",
    "discard = (1 - p_y) * (1-p_x0_y0) * (1-p_x1_y0)* (1-p_x2_y0) * (1-p_x3_y0) * (1-p_x4_y0)\n",
    "\n",
    "if (keep>discard):\n",
    "    print(\"Problem 1: keep \\n\")\n",
    "else:\n",
    "    print(\"Problem 1: discard \\n\")\n",
    "\n",
    "# problem 2: x = (1, 1, 0, 1, 0)\n",
    "keep2 = p_y * p_x0_y1 * p_x1_y1 * (1-p_x2_y1) * p_x3_y1 * (1-p_x4_y1)\n",
    "discard2 = (1 - p_y) * p_x0_y0 * p_x1_y0 * (1-p_x2_y0) * p_x3_y0 * (1-p_x4_y0)\n",
    "\n",
    "if (keep2>discard2):\n",
    "    print(\"Problem 2: keep \\n\")\n",
    "else:\n",
    "    print(\"Problem 2: discard \\n\")\n",
    "        \n",
    "# Joint versus Naive Bayes:\n",
    "# We should not use a joint Bayes classifier because the joint probability table\n",
    "# would have 2^5 = 32 entries and some of the entries would have to be calculated by hand.\n",
    "# This would include accounting for the influence of every feature on every other feature.\n",
    "# After all 32 scenarios have probabilty estimated for them, the training data only has\n",
    "# 10 data points.  This would make it impossible to estimate some of the combinations and \n",
    "# they would have to be estimated by hand.\n",
    "\n",
    "# Classification with Missing Features\n",
    "# Using Naive Bayes, we would not have to retrain the model because all of the predictions are\n",
    "# calculated on the fly.  Since each feature is considered on its own, the predictions would\n",
    "# be calculated exactly the same except the \"know author?\" feature would be omitted.  If this was \n",
    "# a Joint Bayes classifier, then the whole model would have to be retrained and the joint\n",
    "# probabilities would have to be recalculated. This includes recalculating all the joint probabilities\n",
    "# that we do not have sufficient training data to determine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Joint versus Naive Bayes\n",
    "**Why should we probably not use a \"joint\" Bayes classifier (using the joint probability of the features x,\n",
    "as opposed to the conditional independencies assumed by Naive Bayes) for these data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Classification with Missing Features\n",
    "\n",
    "**Suppose that before we make our predictions, we lose access to my address book, so that we cannot tell whether the email author is known.  Do we need to re-train the model to classify based solely on the other four features?  If so, how? What, if anything, changes about the parameters or the way they are used?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
